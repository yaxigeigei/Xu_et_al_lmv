{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import os\n",
    "from os.path import join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 'NP30'\n",
    "block = 'B12'\n",
    "subject_block = '_'.join((subject, block))\n",
    "datatype='neuropixel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_root = Path('/userdata/smetzger/repos/np_preproc/output')\n",
    "if not subject == 'NP11':\n",
    "    kilosort_root = Path(f'/userdata/smetzger/data/{subject}_{block}_mc/')\n",
    "else: \n",
    "    kilosort_root = Path('/userdata/smetzger/data/NP11_B4_mc_update/NP11_B4_data/NP11_B4_mc/')\n",
    "labels_fp = os.path.join(preproc_root, subject_block, 'labels', 'combined_speech_labels.csv')\n",
    "labels = pd.read_csv(labels_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make a mapping from the feature names, to the dir within that feature names output that has results!\n",
    "resdir_dict = {\n",
    "    'acoustic_feats':None,\n",
    "    'artics':'artic_files/F01_indep_Haskins_loss_90_filter_fix_bn_False_0_setting2',\n",
    "    'formants':None,\n",
    "    'phones':'results/speaker1',\n",
    "    'pitch':None,\n",
    "    'spectrograms':None,\n",
    "}\n",
    "\n",
    "resfeat_dict = {\n",
    "    'acoustic_feats':['peakRate', 'peakEnv', 'env'],\n",
    "    'artics':['ttx', 'tty', 'tdx', 'tdy', 'tbx', 'tby', 'lix', 'liy', 'ulx', 'uly', 'llx', 'lly', 'la', \n",
    "             'pro', 'ttcl', 'tbcl', 'vx', 'vy'], #Note the last one is lip aperture. \n",
    "    'formants':['f1', 'f2', 'f3'],\n",
    "    'pitch':['rel-pitch', 'pitchMin', 'pitchMax', 'pitchUp', 'pitchDown'],\n",
    "    'spectrograms':['spec_%d' %k for k in range(80)], # Not sure the frequency bins as of now.\n",
    "    'phones':None # need to use textgrids still for these :D \n",
    "}\n",
    "\n",
    "expected_offsets = {\n",
    "    'formants':0.025\n",
    "}\n",
    "\n",
    "def time_from_resfile(file):\n",
    "    \"\"\"\n",
    "    given a file, e.g. stim_t0_t1.mat etc\n",
    "    returns the t0, t1 tuple as a set of floats.\n",
    "    \"\"\"\n",
    "    t0 = None\n",
    "    t1 = None\n",
    "    if len(file.split('_')) == 3:\n",
    "        t0 = float(file.split('_')[1])\n",
    "        t1 = float('.'.join(file.split('_')[2].split('.')[:2]))\n",
    "    \n",
    "    elif len(file.split('_')) == 2:\n",
    "        t0 = float(file.split('_')[0])\n",
    "        t1 = float('.'.join(file.split('_')[1].split('.')[:2]))\n",
    "        \n",
    "    return t0, t1\n",
    "    \n",
    "basefp  = str(preproc_root/ subject_block)\n",
    "### Adjust the timings based on whats in the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Lets load in the neural data. \n",
    "\n",
    "if datatype == 'neuropixel':\n",
    "    ks_out_fp = str(kilosort_root)\n",
    "    spikes = pd.read_csv(join(kilosort_root, 'cluster_group.tsv'), sep='\\t')\n",
    "    from process_units.extract_spikes_from_kilosort import extract_spikes\n",
    "    neural_array, unit_tags = extract_spikes(ks_out_fp, \n",
    "                                 30e3, \n",
    "                                  ks_offset = 0,\n",
    "                                  spike_cutoff=1000,\n",
    "                                    prcsd_sr = 100, \n",
    "                                        many_clusters=None\n",
    "                                 )\n",
    "else: \n",
    "    ks_out_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape (170172, 66)\n"
     ]
    }
   ],
   "source": [
    "#### This is a dataframe with the phonemes and words from MFA\n",
    "from preproc_func.phoneme_extract.process_textgrids import extract_phonemes, phone_df_to_arr, phone_df_to_word_level_feats\n",
    "phonemes_and_words_df = extract_phonemes(os.path.join(preproc_root, subject_block, 'phones', 'results'), \n",
    "                                         100, neural_array.shape[0])\n",
    "phone_features, phone_enc_dict = phone_df_to_arr(phonemes_and_words_df, 100, neural_array.shape[0], 0)\n",
    "word_onsets, onset_labels, word_offsets, offset_labels = phone_df_to_word_level_feats(phonemes_and_words_df, 100, neural_array.shape[0], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from file_compilation.file_compilation import postprocess_phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### We can derive additional features for free based on the phonemes. \n",
    "# This will output the following features: \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to /home/smetzger/nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved at /userdata/smetzger/repos/np_preproc/output/NP30_B12/labels/combined_speech_labels_fa.csv\n"
     ]
    }
   ],
   "source": [
    "word_feats_df, sent_feats_df, phones_df, stress_df, complexity_df, vowels  = postprocess_phonemes(phonemes_and_words_df, \n",
    "                                  phone_features, phone_enc_dict, \n",
    "                                  word_onsets, \n",
    "                                  onset_labels, \n",
    "                                  word_offsets, \n",
    "                                  offset_labels, neural_array, labels, \n",
    "                                 labels_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Okay, now we can go through each file, and load the file into the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading acoustic_feats\n",
      "498 acoustic_feats\n",
      "3\n",
      "loading artics\n",
      "498 artics\n",
      "18\n",
      "loading formants\n",
      "498 formants\n",
      "3\n",
      "loading phones\n",
      "loading pitch\n",
      "498 pitch\n",
      "5\n",
      "loading spectrograms\n",
      "498 spectrograms\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "resdir_dfs = []\n",
    "for k, v in resdir_dict.items():\n",
    "    print('loading', k)\n",
    "    if k == 'phones':\n",
    "        continue\n",
    "    if v is None: \n",
    "        resfp = os.path.join(basefp, k)\n",
    "    else: \n",
    "        resfp = os.path.join(basefp, k, v)\n",
    "    files = sorted(os.listdir(resfp))\n",
    "    files = [f for f in files if not f == 'results.mat']\n",
    "    print(len(files), k)\n",
    "    print(len(resfeat_dict[k]))\n",
    "    featarray = np.empty((neural_array.shape[0], len(resfeat_dict[k])))\n",
    "    featarray[:] = np.nan\n",
    "    for f in files: \n",
    "        if not f.endswith('.npy'):\n",
    "            continue\n",
    "        data = np.load(os.path.join(resfp, f))\n",
    "        t0, _ = time_from_resfile(f)\n",
    "        ind = int(t0*100)\n",
    "        end = ind + data.shape[0]\n",
    "        featarray[ind:end] = data\n",
    "    resdir_dfs.append(pd.DataFrame(data=featarray, columns=resfeat_dict[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = pd.concat(resdir_dfs + list([word_feats_df, sent_feats_df, phones_df, stress_df, complexity_df]) \n",
    "                        , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_df = pd.DataFrame(data=neural_array, columns = unit_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = pd.concat((features_df, neural_df), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df['time (s)'] = np.arange(len(features_df))/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'c_arr', 'h_arr', 'll_arr', 't_arr', 'ul_arr', 'w_arr'])\n"
     ]
    }
   ],
   "source": [
    "from file_compilation.file_compilation import extract_mxu_vid_feats\n",
    "if subject in ['NP30', 'NP11']:\n",
    "    video_derived_df = extract_mxu_vid_feats(subject)\n",
    "    features_df.index = features_df['time (s)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if subject in ['NP30', 'NP11']:\n",
    "    features_df= features_df.join(video_derived_df.set_index('t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df= features_df.rename(columns= {u:'unit_%d' %u for u in unit_tags})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.index = np.arange(len(features_df))/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list((features_df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.to_hdf(os.path.join(preproc_root, subject_block, subject_block+ '_feats.h5'), key='df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spikeint",
   "language": "python",
   "name": "spkint"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
