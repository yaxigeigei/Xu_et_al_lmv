{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task data preprocessing notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Experiment info\n",
    "subject = 'NP122'\n",
    "block = 'B1'\n",
    "subject_block = '_'.join([subject, block])\n",
    "\n",
    "# Shared paths\n",
    "preproc_root = Path('/data_store2/neuropixels/preproc')\n",
    "label_dir = preproc_root / subject_block / 'labels'\n",
    "chunk_wav_dir = preproc_root / subject_block / 'audio_files' / 'clips'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Extract and denoise audio files**\n",
    "### **1.1. Read mic and speaker channels from NIDQ and save them as WAV files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from audio import save_ni_audio\n",
    "\n",
    "save_ni_audio(preproc_root, subject_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **1.2. USER INTERVENTION: Denoise microphone audio in Audacity**\n",
    "\n",
    "This part IS MANDATORY\n",
    "\n",
    "1) Open the *_mic.wav file in Audacity.\n",
    "2) Select a period of noise.\n",
    "3) In the menu bar, go to Effect (-> Noise Removal and Repair) -> Noise Reduction. There's a slight difference in menu organization in different versions of Audacity.\n",
    "4) Click Get Noise Profile. (Click the question mark button at the bottom right corner of the dialog for more instruction.)\n",
    "5) Select the entire audio track.\n",
    "5) Go to Effect -> Noise Reduction, and click OK to apply noise reduction.\n",
    "8) File -> Export -> Export as WAV, and save the audio file as *_mic_denoised.wav in the same folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Extract speech and task event labels**\n",
    "### **2.1. Create the task_timing.csv file**\n",
    "#### This spreadsheet has four columns\n",
    "1) Task name - we currently support sentgen and LMV, but can add additional task extractors flexibly. \n",
    "2) Task start - start time in seconds\n",
    "3) Task end - end times in second\n",
    "4) Curated - if the labels have already been corrected. If so, will skip the task extraction. Useful for adding additional features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Edit and specify task timing\n",
    "if subject_block in ['NP32_B2', 'NP32_B3', 'NP41_B1', 'NP41_B2', 'NP38_B6']:\n",
    "    task_timing = [('lmv', 0, np.inf)]\n",
    "elif subject_block == 'NP35_B1': task_timing = [('lmv', 32, 400)]\n",
    "elif subject_block == 'NP35_B2': task_timing = [('lmv', 110, 730)]\n",
    "elif subject_block == 'NP40_B1': task_timing = [('semsr', 425, 770)]\n",
    "elif subject_block == 'NP40_B2': task_timing = [('semsr', 156, 476)]\n",
    "elif subject_block == 'NP43_B1': task_timing = [('lmv', 0, 7*60+50), ('timit', 8*60+35, 13*60+31)]\n",
    "elif subject_block == 'NP44_B2': task_timing = [('lmv', 7*60+20, 18*60)]\n",
    "elif subject_block == 'NP44_B3': task_timing = [('lmv', 2*60+15, 10*60+13)]\n",
    "elif subject_block == 'NP45_B1': task_timing = [('lmv', 3*60+7, 11*60+3)]\n",
    "elif subject_block == 'NP45_B2': task_timing = [('lmv', 2*60+20, 10*60+42)]\n",
    "elif subject_block == 'NP46_B1': task_timing = [('lmv', 4*60+45, 12*60+30)]\n",
    "elif subject_block == 'NP47_B3': task_timing = [('lmv', 2*60, 10*60+40)]\n",
    "elif subject_block == 'NP50_B2': task_timing = [('timit', 2*60+45, 6*60+9)]\n",
    "elif subject_block == 'NP50_B3': task_timing = [('timit', 82, 554), ('lmv', 637, 1164)]\n",
    "elif subject_block == 'NP51_B1': task_timing = [('lmv', 2*60+34, 10*60)]\n",
    "elif subject_block == 'NP52_B1': task_timing = [('lmv', 197, 771)]\n",
    "elif subject_block == 'NP53_B1': task_timing = [('lmv', 94, 512), ('semsr', 668, 1164)]\n",
    "elif subject_block == 'NP54_B1': task_timing = [('lmv', 285, 852)]\n",
    "elif subject_block == 'NP55_B1': task_timing = [('lmv', 150, 632), ('semsr', 767, 1134)]\n",
    "elif subject_block == 'NP55_B2': task_timing = [('semsr', 147, 750)]\n",
    "elif subject_block == 'NP56_B1': task_timing = [('lmv', 271, 1537)]\n",
    "elif subject_block == 'NP57_B1': task_timing = [('semsr', 580, 18*60+30), ('timit', 20*60, 28*60)]\n",
    "elif subject_block == 'NP57_B2': task_timing = [('timit', 12, 467)]\n",
    "elif subject_block == 'NP58_B2': task_timing = [('lmv', 350, 16*60), ('cv', 16*60+50, 19*60+10)]\n",
    "elif subject_block == 'NP59_B1': task_timing = [('semsr', 239, 764)]#, ('dimex', 842, 1262)]\n",
    "elif subject_block == 'NP59_B2': task_timing = [('semsr', 154, 662)]\n",
    "elif subject_block == 'NP60_B1': task_timing = [('lmv', 239, 975)]\n",
    "elif subject_block == 'NP61_B1': task_timing = [('semsr', 1.5, 770)]\n",
    "elif subject_block == 'NP62_B1': task_timing = [('semsr', 560, 794)]\n",
    "elif subject_block == 'NP62_B2': task_timing = [('semsr', 224, 806)]\n",
    "elif subject_block == 'NP64_B1': task_timing = [('semsr', 191, 552), ('lmv', 689, 1238)]\n",
    "elif subject_block == 'NP65_B2': task_timing = [('semsr', 11, 637)]\n",
    "elif subject_block == 'NP66_B1': task_timing = [\n",
    "    # ('dimex_intraop_s1', 383, 561), \n",
    "    # ('dimex_intraop_s5', 568, 794), \n",
    "    ('timit', 839, 1283)\n",
    "]\n",
    "elif subject_block == 'NP66_B2': task_timing = [\n",
    "    # ('dimex_intraop_s1', 327, 505), \n",
    "    # ('dimex_intraop_s5', 509, 735), \n",
    "    ('timit', 745, 1187), \n",
    "    # ('dimex_s2', 1244, 1509)\n",
    "]\n",
    "elif subject_block == 'NP67_B1': task_timing = [('timit', 423, 868), ('semsr', 1118 , 1855), ('timit', 2020, 2465)]\n",
    "elif subject_block == 'NP68_B1': task_timing = [('timit', 161, 470)]\n",
    "elif subject_block == 'NP69_B1': task_timing = [('lmv', 295, 777), ('semsr', 870, 1259)]\n",
    "elif subject_block == 'NP69_B2': task_timing = [('lmv', 136, 628), ('semsr', 672, 1014)]\n",
    "elif subject_block == 'NP70_B2': task_timing = [('semsr', 29, 660)]\n",
    "elif subject_block == 'NP71_B2': task_timing = [('timit', 0, 8*60),] #('semsr', ?, ?)]\n",
    "elif subject_block == 'NP72_B1': task_timing = [('timit', 280, 750), ('natstim', 810, 1550, [1, 2])]\n",
    "elif subject_block == 'NP72_B2': task_timing = [('timit', 3*60+2, 10*60+24), ('natstim', 11*60+22, 18*60+5, [3])]\n",
    "elif subject_block == 'NP73_B1': task_timing = [('timit', 6*60+40, 14*60+3)]\n",
    "elif subject_block == 'NP74_B1': task_timing = [('lmv', 225, 694), ('cv', 783, 887)]\n",
    "elif subject_block == 'NP76_B1': task_timing = [\n",
    "    #('dimex', 3*60, 10*60+15), \n",
    "    ('timit', 890, 1380)]\n",
    "elif subject_block == 'NP77_B2': task_timing = [('arithmetic', 26, 1031)]\n",
    "elif subject_block == 'NP78_B1': task_timing = [('lmv', 892, 1363), ('semsr', 1445, 2141)]\n",
    "elif subject_block == 'NP79_B1': task_timing = [('timit', 162, 606)]\n",
    "elif subject_block == 'NP79_B2': task_timing = [('timit', 291, 731), ('semsr', 852, 1956)]\n",
    "elif subject_block == 'NP80_B1': task_timing = [('semsr', 207, 958), ('lmv', 1048, 1555)]\n",
    "elif subject_block == 'NP81_B1': task_timing = [('lmv', 315, 777)]\n",
    "elif subject_block == 'NP85_B1': task_timing = [('semsr', 367, 961)]\n",
    "elif subject_block == 'NP85_B3': task_timing = [('semsr', 111, 649), ('arithmetic', 700, 1060)]\n",
    "elif subject_block == 'NP86_B1': task_timing = [('semsr', 282, 752)]\n",
    "elif subject_block == 'NP87_B1': task_timing = [('natstim', 233, 937, [1, 2]), ('semsr', 1093, 1560)]\n",
    "elif subject_block == 'NP88_B1': task_timing = [('semsr', 66, 520)]\n",
    "elif subject_block == 'NP88_B2': task_timing = [('semsr', 175, 1031)]\n",
    "elif subject_block == 'NP89_B1': task_timing = [('semsr', 308, 804), ('arithmetic', 972, 1434)]\n",
    "elif subject_block == 'NP90_B1': task_timing = [('semsr', 719, 1220)]\n",
    "elif subject_block == 'NP90_B2': task_timing = [('semsr', 428, 831)]\n",
    "elif subject_block == 'NP90_B4': task_timing = [('semsr', 10, 401)]\n",
    "elif subject_block == 'NP91_B1': task_timing = [('semsr', 398, 961), ('arithmetic', 1030, 1542)]\n",
    "elif subject_block == 'NP92_B2': task_timing = [('arithmetic', 339, 933)]\n",
    "elif subject_block == 'NP93_B1': task_timing = [('timit', 321, 764), ('semsr', 855, 1261)]\n",
    "elif subject_block == 'NP94_B1': task_timing = [('lmv', 257, 508), ('lmv', 533, 764), ('arithmetic', 844, 1393), ('semsr', 1515, 2065)]\n",
    "elif subject_block == 'NP95_B1': task_timing = [('semsr', 268,1162)]\n",
    "elif subject_block == 'NP96_B1': task_timing = [('sentgen', 645, 1014)]\n",
    "elif subject_block == 'NP97_B1': task_timing = [('sentgen', 266, 1117), ('semsr', 1300, 1838)]\n",
    "elif subject_block == 'NP98_B1': task_timing = [('timit', 205, 650)]\n",
    "elif subject_block == 'NP101_B3': task_timing = [('timit', 147, 584)]\n",
    "elif subject_block == 'NP102_B2': task_timing = [('nbd_listen', 800, 880), ('ptb_read', 936, 1066), ('nbd_listen', 1088, 1170), ('ptb_read', 1188, 1325)]\n",
    "elif subject_block == 'NP104_B2': task_timing = [('nbd_listen', 204, 281), ('ptb_read', 314, 483), ('nbd_listen', 525, 611), ('ptb_read', 641, 802)]\n",
    "elif subject_block == 'NP105_B1': task_timing = [('semsr', 330, 940)]\n",
    "elif subject_block == 'NP106_B1': task_timing = [('lmv', 176, 881), ('lmv', 979, 1445)] # ('bdg', 1473, 1512)\n",
    "elif subject_block == 'NP108_B1': task_timing = [('nbd', 250, 250 + 534.706)]\n",
    "elif subject_block == 'NP111_B1': task_timing = [('ptb_read', 357, 2008)] #, ('dimex', 2087, 2272)]\n",
    "elif subject_block == 'NP112_B1': task_timing = [('lmv', 260, 890), ('ptb_read', 1060, 1470)]\n",
    "elif subject_block == 'NP113_B1': task_timing = [('lmv', 140, 1043), ('custom_cv', 1043, 1538), ('mocha', 1538, 1837)]\n",
    "elif subject_block == 'NP114_B1': task_timing = [('nbd', 0, 1136), ('ptb_read', 1490, 1550)] #, ('lmv', 1688, 2602)]\n",
    "elif subject_block == 'NP116_B1': task_timing = [('nbd', 0, 750), ('semsr', 1380, 1864)]\n",
    "elif subject_block == 'NP117_B1': task_timing = [('nbd', 419, 960), ('ptb_read', 1017, 2195)]\n",
    "elif subject_block == 'NP118_B2': task_timing = [('semsr', 182, 763), ('sentgen', 1044, 1350)]\n",
    "elif subject_block == 'NP119_B1': task_timing = [('lmv', 254, 1085), ('sentgen', 1890, 2208), ('mocha', 1320, 1725), ('custom_cv', 2310, 2550)]\n",
    "elif subject_block == 'NP120_B1': task_timing = [('lmv', 400, 953), ('nbd', 1128, 1774), ('ptb_read', 1935, 2571), ('nbd', 2747, 3060)]\n",
    "elif subject_block == 'NP122_B1': task_timing = [('sentgen', 600, 1015), ('semsr', 1305, 1718), ('lmv', 1783, 2564)]\n",
    "else: task_timing = [('', 0, np.inf)]\n",
    "\n",
    "# Create dataframe\n",
    "task_timing_df = pd.DataFrame({\n",
    "    'Task name': [k[0] for k in task_timing],\n",
    "    'Task start': [k[1] for k in task_timing],\n",
    "    'Task end': [k[2] for k in task_timing],\n",
    "    'Curated': [False for k in task_timing]\n",
    "})\n",
    "\n",
    "# Save dataframe if not already\n",
    "label_dir.mkdir(parents=True, exist_ok=True)\n",
    "task_timing_file =  label_dir / 'task_timing.csv'\n",
    "if False and not task_timing_file.exists():\n",
    "    task_timing_df.to_csv(task_timing_file)\n",
    "    print('Saved', task_timing_file)\n",
    "else:\n",
    "    task_timing_df.to_csv(task_timing_file)\n",
    "    print('Not overwriting the existing task_timing.csv file')\n",
    "\n",
    "# Read the task_timing.csv file\n",
    "task_timing_df = pd.read_csv(task_timing_file)\n",
    "print(task_timing_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.2. Run task extractor on each task**\n",
    "\n",
    "Go through each task in the task_timing.csv and get out event timing using that task's timing extractor\n",
    "\n",
    "If the Curated column of a task is True, then we skip extracting that task's timing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.io.wavfile import read\n",
    "from task_extractor.lmv_extractor import LMVExtractor\n",
    "from task_extractor.timit_extractor import TIMITExtractor\n",
    "from task_extractor.semsr_extractor import SemsrExtractor\n",
    "from task_extractor.rep_extractor import RepExtractor\n",
    "from task_extractor.natstim_extractor import NatStimExtractor\n",
    "from task_extractor.sentgen_extractor import SentGenExtractor\n",
    "from task_extractor.mocha_extractor import MOCHAExtractor\n",
    "\n",
    "# Reload the task_timing.csv file\n",
    "task_timing_df = pd.read_csv(task_timing_file)\n",
    "\n",
    "# Read denoised mic audio\n",
    "PROD_TASKS = [\"mocha\", \"cv\", \"semsr\", \"lmv\", \"sentgen\"]\n",
    "if np.any([task_name in PROD_TASKS for task_name in task_timing_df['Task name'].unique()]):\n",
    "    mic_wav_file = preproc_root / subject_block / 'audio_files' / (subject_block + '_mic_denoised.wav')\n",
    "else:\n",
    "    mic_wav_file = preproc_root / subject_block / 'audio_files' / (subject_block + '_mic.wav')\n",
    "\n",
    "sample_rate, mic = read(mic_wav_file)\n",
    "\n",
    "# Read speaker audio\n",
    "speaker_wav_file = preproc_root / subject_block / 'audio_files' / (subject_block + '_speaker.wav')\n",
    "sample_rate, speaker = read(speaker_wav_file)\n",
    "\n",
    "pdiode_wav_file = preproc_root / subject_block / 'audio_files' / (subject_block + '_pdiode.wav')\n",
    "sample_rate_pdiode, pdiode = read(pdiode_wav_file)\n",
    "\n",
    "# Iterate through tasks\n",
    "for task_name in task_timing_df['Task name'].unique():\n",
    "    # Get time window(s)\n",
    "    is_task = task_timing_df['Task name'] == task_name\n",
    "    t_win = list(zip(task_timing_df['Task start'][is_task], task_timing_df['Task end'][is_task]))\n",
    "    \n",
    "    # Skip extraction if results are already curated\n",
    "    if np.any(task_timing_df['Curated'][is_task] == True):\n",
    "        print('Skip '+task_name+' since the results have been curated')\n",
    "        continue\n",
    "    \n",
    "    # Run extractor\n",
    "    task_name = task_name.lower()\n",
    "    if task_name == 'lmv':\n",
    "        print('Run '+task_name+' extractor')\n",
    "        lmv = LMVExtractor(preproc_root, subject_block, t_win)\n",
    "        lmv.extract_cue(speaker, sample_rate)\n",
    "        lmv.extract_stim(speaker, sample_rate)\n",
    "        if subject_block in ['NP58_B2']: lmv.extract_prod(mic, sample_rate, th=10.0)\n",
    "        else: lmv.extract_prod(mic, sample_rate)\n",
    "        lmv.write_timing_files()\n",
    "        \n",
    "    elif task_name == 'timit':\n",
    "        print('Run '+task_name+' extractor')\n",
    "        timit = TIMITExtractor(preproc_root, subject_block, t_win)\n",
    "        timit.extract_cue(speaker, sample_rate)\n",
    "        timit.extract_stim(speaker, sample_rate, max_gap=0.31)\n",
    "        timit.write_timing_files()\n",
    "        \n",
    "    elif task_name == 'semsr':\n",
    "        print('Run '+task_name+' extractor')\n",
    "        semsr = SemsrExtractor(preproc_root, subject_block, t_win)\n",
    "        semsr.extract_stim(speaker, sample_rate)\n",
    "        semsr.extract_prod(mic, sample_rate)\n",
    "        semsr.write_timing_files()\n",
    "        \n",
    "    elif task_name == 'cv':\n",
    "        print('Run rep extractor for'+task_name)\n",
    "        rep = RepExtractor(preproc_root, subject_block, t_win, task_name=task_name)\n",
    "        rep.extract_stim(speaker, sample_rate)\n",
    "        rep.extract_prod(mic, sample_rate)\n",
    "        rep.write_timing_files()\n",
    "        \n",
    "    elif task_name == \"natstim\":\n",
    "        print('Run rep extractor for'+task_name)\n",
    "        # Note this should be the block order run in the OR!\n",
    "        # PLease change as appropriate!\n",
    "        block_order = [1, 2]\n",
    "        natstim = NatStimExtractor(preproc_root, subject_block, t_win, block_order)\n",
    "        natstim.extract_cue(speaker, sample_rate)\n",
    "        natstim.extract_stim(speaker, sample_rate, max_gap=2)\n",
    "        natstim.write_timing_files()\n",
    "        \n",
    "    elif task_name == 'sentgen':\n",
    "        sentgen = SentGenExtractor(preproc_root, subject_block, t_win)\n",
    "        # If using the pdiode (speaker fails) set to True + provide the guess time of where the first image appears\n",
    "        sentgen.extract_fixation(speaker, sample_rate, pdiode=False, guess=600.852)\n",
    "        sentgen.extract_prod(mic, sample_rate)\n",
    "        # Modify ending as kissing ('ing') or kisses ('s')\n",
    "        # Modify the presence of 'the' at the beginning (The boy kisses vs Boy kisses)\n",
    "        # Modify the presence of 'being' (The boy is being kicked vs The boy is kicked)\n",
    "        sentgen.write_timing_files(ending='s', the='The', being='')\n",
    "        \n",
    "    elif task_name == 'mocha':\n",
    "        continue # Requires manual intervention, skip for now\n",
    "        mocha = MOCHAExtractor(preproc_root, subject_block, t_win)\n",
    "        mocha.extract_events(pdiode, sample_rate_pdiode, mode='log', guess=1331.698)\n",
    "        mocha.extract_prod(mic, sample_rate)\n",
    "        mocha.write_timing_files()\n",
    "        mocha.process_stim_timing_mocha(label_dir / 'mocha' / 'stim_timing.txt')\n",
    "\n",
    "    elif task_name == 'dimex':\n",
    "        print('DIMEX has not been implemented as a taskextractor')\n",
    "        \n",
    "    elif task_name == 'arithmetic':\n",
    "        print('ARITHMETIC has not been implemented as a taskextractor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### **2.3. USER INTERVENTION: Refine speech onsets and offsets in Audacity**\n",
    "\n",
    "Only files that end with auto.txt need to be refined. The other files should already be correct. \n",
    "\n",
    "1) Add labels to soundtrack via File -> Import -> Labels, selecting the speech_*_timing_auto.txt file\n",
    "2) Examine and adjust spNatStimExtractorch onsets and offsets\n",
    "2) Correct labels if what's spoken was different\n",
    "3) Add new labels for additional utterances that may be useful for analysis\n",
    "4) Save the curated labels via File -> Export -> Export Labels. The name should now be speech_*_timing.txt without the word 'auto'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **2.4. Combine labels across tasks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from file_org import label_combine\n",
    "\n",
    "task_names = set(task_timing_df['Task name'].values)\n",
    "\n",
    "label_combine(label_dir, task_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Cut speech audio into clips based on onset and offset times**\n",
    "\n",
    "Depending on the Source, audio clips are extracted from either speaker (for 'stim') or microphone (for other types) audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from audio import chunk_audio\n",
    "\n",
    "speech_df = pd.read_csv(label_dir / 'combined_speech_labels.csv')\n",
    "speaker_df = speech_df.loc[speech_df['Source'] =='stim'] # Labels to use from the speaker\n",
    "mic_df = speech_df.loc[speech_df['Source'] != 'stim']\n",
    "\n",
    "speaker_wav_file = preproc_root / subject_block / 'audio_files' / (subject_block + '_speaker.wav')\n",
    "mic_wav_file = preproc_root / subject_block / 'audio_files' / (subject_block + '_mic_denoised.wav')\n",
    "\n",
    "!rm -rf $chunk_wav_dir # clear the directory\n",
    "\n",
    "if len(speaker_df) > 0:\n",
    "    chunk_audio(speaker_df,\n",
    "                speaker_wav_file,\n",
    "                chunk_wav_dir,\n",
    "                label_offset=0,\n",
    "                audio_offset=0,\n",
    "                plot_interval=30)\n",
    "\n",
    "if len(mic_df) > 0:\n",
    "    chunk_audio(mic_df,\n",
    "                mic_wav_file,\n",
    "                chunk_wav_dir,\n",
    "                label_offset=0,\n",
    "                audio_offset=0,\n",
    "                plot_interval=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **4. Extract features from audio**\n",
    "\n",
    "Most of the following subsections can be excuted on-demand and in parallel.\n",
    "The exception is the pitch extraction in 4.4, which requires extracted phonetic labels from 4.2.\n",
    "\n",
    "### **4.1. Estimate articulatory trajectories**\n",
    "\n",
    "This uses the bootphon articulatory inversion package. For more information check out their github. https://github.com/bootphon/articulatory_inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from aai import invert\n",
    "\n",
    "artics_dir = preproc_root / subject_block / 'artics'\n",
    "\n",
    "!rm -rf $artics_dir\n",
    "\n",
    "invert(chunk_wav_dir, artics_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.2. Get peakRate, peakEnv, and env using Yulia's peakRate code**\n",
    "\n",
    "Run a script that does the matlab peakRate extraction. The results will come out at 100 samples/sec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from intensity.oganian_2019 import extract_intensity\n",
    "\n",
    "intens_dir = preproc_root / subject_block / 'intensity'\n",
    "\n",
    "!rm -rf intens_dir\n",
    "\n",
    "extract_intensity(chunk_wav_dir, intens_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.3. Extract phonetic labels**\n",
    "\n",
    "This uses montreal-forced-aligner. Please ask Many for installation instruction if you need them.\n",
    "\n",
    "NOTE (03/14/2024): You don't need to install MFA, Quinn's environment will work. But, you do need to copy the pretrained model to your `$MFA_ROOT_DIR`. This directory is by default `~/Documents/MFA`. Copy the files in `/home/qgreicius/Documents/MFA/pretrained_models/acoustic` to `$MFA_ROOT_DIR/pretrained_models/acoustic`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from phoneme.mfa import run_mfa\n",
    "\n",
    "speech_df = pd.read_csv(label_dir / 'combined_speech_labels.csv')\n",
    "phone_dir = preproc_root / subject_block / 'phones'\n",
    "print(speech_df, phone_dir, chunk_wav_dir)\n",
    "!rm -rf $phone_dir\n",
    "run_mfa(chunk_wav_dir, speech_df, phone_dir, language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Log files are generated in your home directory\n",
    "# If this fails, check the note in the subsection description above.\n",
    "mfa_log_dir = Path(os.path.expanduser(\"~\")) / 'Documents/MFA/phones_validate_pretrained'\n",
    "log_files = []\n",
    "log_files.append(mfa_log_dir / 'validate_pretrained.log')\n",
    "log_files.append(mfa_log_dir / 'oovs_found_librispeech_lex.txt')\n",
    "log_files.append(mfa_log_dir / 'unalignable_files.csv')\n",
    "\n",
    "for log_file in log_files:\n",
    "    try:\n",
    "        with open(log_file, 'r') as file:\n",
    "            content = file.read()\n",
    "            print(content)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File '{log_file}' not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell to generate reading textgrids for SentGen and MOCHA tasks\n",
    "from genTextGrid import generate_textgrid\n",
    "for task in task_timing_df['Task name'].unique():\n",
    "    if task in ['mocha', 'sentgen']:\n",
    "        stimLabels = label_dir / task / 'reading_stim_timing.txt'\n",
    "        df = pd.read_csv(stimLabels, sep='\\t', header=None, names=['start_time', 'end_time', 'text'])\n",
    "        for _, row in df.iterrows():\n",
    "            generate_textgrid(subject_block, row['text'], row['start_time'], row['end_time'], preproc_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If `NatStim` was run, replace the textgrids created by MFA with the manually corrected versions.\n",
    "# NOTE: Atm, this only replaces the `speaker` dirs and not the `all` dir in `{phone_dir}/results`.\n",
    "if np.any(np.array([tt[0] for tt in task_timing])==\"natstim\"):\n",
    "    natstim.set_corrected_textgrids(speech_df, phone_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Before proceeding**\n",
    "#### Always check if there are any unaligned clips. If there are that usually means some labels were off. Also check and fix any OOVs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoneme.mfa import pool_speakers\n",
    "\n",
    "pool_speakers(phone_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.4. Extract pitch features**\n",
    "#### NOTE: THIS STEP REQUIRES THE COMPLETION OF 4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pitch.straight import extract_pitch\n",
    "\n",
    "tg_dir = preproc_root / subject_block / 'phones/results/all'\n",
    "pitch_dir = preproc_root / subject_block / 'pitch'\n",
    "\n",
    "!rm -rf $pitch_dir\n",
    "\n",
    "extract_pitch(chunk_wav_dir, tg_dir, pitch_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEGUE - Change permissions on all dirs created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cmd = f\"find /data_store2/neuropixels/preproc/{subject_block} -user $(whoami)\" + \" -exec chgrp neuropixels {} + -exec chmod 775 {} +\"\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **4.5. Extract formants using PRAAT**\n",
    "#### WARNING: THIS STEP REQUIRES USER INTERVENTION AS THE SCRIPT HAS TO BE RUN LOCALLY\n",
    "it'd be great if it didnt :sob:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Change the praat template: \n",
    "# Set the Mount point (this will need to be run locally)\n",
    "newls = []\n",
    "numbered_wav_fp = str(numbered_wavdir) +'/'\n",
    "with open('./preproc_func/formants/praat_template.txt', 'r') as f: \n",
    "    l = f.readlines()\n",
    "    newls = [ll.replace('{filepath}', numbered_wav_fp) for ll in l]\n",
    "    newls = [ll.replace('{mount_point}', mount_point) for ll in newls]\n",
    "    \n",
    "with open(f'./preproc_func/formants/praat_scripts/praat_{subject}_{block}.txt', 'w') as f: \n",
    "    f.writelines(newls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructions for formant extraction from here: \n",
    "1. Open Praat\n",
    "2. On mac, click on Praat, and hit Open Praat Script\n",
    "3. Load up the script\n",
    "4. Hit run script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formant_fp = preproc_root /  subject_block / 'formants'\n",
    "!rm -rf $formant_fp\n",
    "formant_fp.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from formants.postprocess_formants import postprocess_formants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from file_org import rename_files\n",
    "postprocess_formants(numbered_wavdir, formant_fp)\n",
    "rename_files(str(formant_fp), ind_to_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.6. Get spectrograms**\n",
    "- Please note that this can be pretty slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_dir = preproc_root / subject_block / 'spectrograms' \n",
    "# !rm -rf $spec_dir\n",
    "spec_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the audio files...\n",
    "# Run the TIMIT spectrogram extraction code on em. \n",
    "from spectrograms.run_spectrogram_extraction import setup_spectrogram_script\n",
    "setup_spectrogram_script(str(chunk_wav_dir),str(spec_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the results: \n",
    "from scipy.io import loadmat\n",
    "\n",
    "outstruct = loadmat(os.path.join(spec_dir, 'results.mat'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = outstruct['out'][0]\n",
    "\n",
    "for dind in range(len(data)):\n",
    "    if data[dind][0].shape[-1] == 0: \n",
    "        continue\n",
    "    else:\n",
    "        spec = data[dind][0].T\n",
    "        file = data[dind][1][0]\n",
    "        start_time = float(file.split('_')[1])\n",
    "        end_time = float(file.split('_')[-1].replace('.wav', '')) \n",
    "        np.save(os.path.join(spec_dir, ('stim_%.3f_%.3f' %(start_time, end_time))), spec[100:-100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **4.7 Timitification of the features (get them to match the TIMIT output files (place, manner labels mostly)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Collect all the files you created, and read them out!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_timings(file, start, end, expected_offset):\n",
    "    \"\"\"\n",
    "    Inputs: file - the full filepath\n",
    "    start - the start time that the file currently has\n",
    "    end - the end time that the file currently has\n",
    "    expected_offsets - how far off we routinely expect the file to be - this is a dict.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        arr = np.load(file)\n",
    "    except Exception:\n",
    "        print('could not load file', file)\n",
    "        return\n",
    "    expected_shape = int(np.round(end-start, 2)*100)\n",
    "   \n",
    "    if not np.max(arr.shape) == expected_shape:\n",
    "        start = start + expected_offset\n",
    "        new_end = np.round(start, 3) + np.max(arr.shape)/100\n",
    "        base = file.split('/')[:-1]\n",
    "        base = '/'.join(base)\n",
    "        newfile = 'stim_%.3f_%.3f.npy' %(start, new_end)\n",
    "        newfile = str(os.path.join(base, newfile))\n",
    "        os.rename(str(file),  newfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a mapping from the feature names, to the dir within that feature names output that has results!\n",
    "resdir_dict = {\n",
    "    'acoustic_feats':None,\n",
    "    'artics':'artic_files/F01_indep_Haskins_loss_90_filter_fix_bn_False_0_setting2',\n",
    "    'formants':None,\n",
    "    'phones':'results/speaker1',\n",
    "    'pitch':None,\n",
    "    'spectrograms':None,\n",
    "}\n",
    "\n",
    "resfeat_dict = {\n",
    "    'acoustic_feats':['peakRate', 'peakEnv', 'env', 'F0_raw'],\n",
    "    'artics':['ttx', 'tty', 'tdx', 'tdy', 'tbx', 'tby', 'lix', 'liy', 'ulx', 'uly', 'llx', 'lly', 'la', \n",
    "             'pro', 'tbcl', 'vx', 'vy'], #Note the last one is lip aperture. \n",
    "    'formants':['f1', 'f2', 'f3'],\n",
    "    'pitch':['rel-pitch', 'pitchMin', 'pitchMax', 'pitchUp', 'pitchDown'],\n",
    "    'spectrograms':['spec_%d' %k for k in range(80)], # Not sure the frequency bins as of now.\n",
    "    'phones':None # need to use textgrids still for these :D \n",
    "}\n",
    "\n",
    "expected_offsets = {\n",
    "    'formants':0.025\n",
    "}\n",
    "\n",
    "def time_from_resfile(file):\n",
    "    \"\"\"\n",
    "    given a file, e.g. stim_t0_t1.mat etc\n",
    "    returns the t0, t1 tuple as a set of floats.\n",
    "    \"\"\"\n",
    "    t0 = None\n",
    "    t1 = None\n",
    "    if len(file.split('_')) == 3:\n",
    "        t0 = float(file.split('_')[1])\n",
    "        t1 = float('.'.join(file.split('_')[2].split('.')[:2]))\n",
    "    \n",
    "    elif len(file.split('_')) == 2:\n",
    "        t0 = float(file.split('_')[0])\n",
    "        t1 = float('.'.join(file.split('_')[1].split('.')[:2]))\n",
    "        \n",
    "    return t0, t1\n",
    "    \n",
    "basefp  = str(preproc_root/ subject_block)\n",
    "\n",
    "# Housekeeping - correct filetimes\n",
    "\n",
    "for k, v in resdir_dict.items():\n",
    "    if v is None: \n",
    "        resfp = os.path.join(basefp, k)\n",
    "    else: \n",
    "        resfp = os.path.join(basefp, k, v)\n",
    "    files = sorted(os.listdir(resfp))\n",
    "    files = [f for f in files if not f == 'results.mat']\n",
    "    features_loaded = resfeat_dict[k]\n",
    "    if not k == 'phones':\n",
    "        for file in files:\n",
    "            adjust_timings(os.path.join(resfp,  file), \n",
    "                           time_from_resfile(file)[0], time_from_resfile(file)[1], \n",
    "                          expected_offsets.get(k, 0))\n",
    "\n",
    "    ### Adjust the timings based on whats in the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in resdir_dict.items():\n",
    "    if not k == 'pitch':\n",
    "        continue\n",
    "    if v is None: \n",
    "        resfp = os.path.join(basefp, k)\n",
    "    else: \n",
    "        resfp = os.path.join(basefp, k, v)\n",
    "    files = sorted(os.listdir(resfp))\n",
    "    files = [f for f in files if not f == 'results.mat']\n",
    "    print(len(files))\n",
    "    break\n",
    "    features_loaded = resfeat_dict[k]\n",
    "    if not k == 'phones':\n",
    "        for file in files:\n",
    "            adjust_timings(os.path.join(resfp,  file), \n",
    "                           time_from_resfile(file)[0], time_from_resfile(file)[1], \n",
    "                          expected_offsets.get(k, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_si_39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
